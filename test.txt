Rate Limiting
If you’ve ever worked with an API for a service, you’ve likely had to contend with rate limiting, which constrains the number of times some kind of resource is accessed to some finite number per unit of time. The resource can be anything: API connections, disk reads/writes, network packets, errors.
Have you ever wondered why services put rate limits in place? Why not allow unfettered access to a system? The most obvious answer is that by rate limiting a system, you prevent entire classes of attack vectors against your system. If malicious users can access your system as quickly as their resources allow it, they can do all kinds of things.
For example, they could fill up your service’s disk either with log messages or valid requests. If you’ve misconfigured your log rotation, they could even perform some‐ thing malicious and then make enough requests that any record of the activity would be rotated out of the log and into /dev/null. They could attempt to brute-force access to a resource, or maybe they would just perform a distributed denial of service attack. The point is: if you don’t rate limit requests to your system, you cannot easily secure it.
Malicious use isn’t the only reason. In distributed systems, a legitimate user could degrade the performance of the system for other users if they’re performing opera‐ tions at a high enough volume, or if the code they’re exercising is buggy. This can even cause the death-spirals we discussed earlier. From a product standpoint, this is awful! Usually you want to make some kind of guarantees to your users about what kind of performance they can expect on a consistent basis. If one user can affect that agreement, you’re in for a bad time. A user’s mental model is usually that their access to the system is sandboxed and can neither affect nor be affected by other users’ activ‐ ities. If you break that mental model, your system can feel like it’s not well engineered, and even cause users to become angry or leave.
Even with only one user, rate limits can be advantageous. A lot of the time, systems have been developed to work well under the common use case, but may begin behav‐ ing differently under different circumstances. In complicated systems such as distributed systems, this effect can cascade through the system and have drastic, unintended consequences. Maybe under load you begin dropping packets, which causes your distributed database to lose its quorum and stop accepting writes, which causes your existing requests to fail, which causes... You can see how this can be a bad thing. It isn’t unheard of for systems to perform a kind of DDoS attack on them‐ selves in these instances!
A Story from the Field
I once worked on a distributed system that scaled work in parallel by starting new processes (this allowed it to scale horizontally to multiple machines). Each process would open a database connection, read some data, and do some calculations. For a time, we had great success in scaling the system in this manner to meet the needs of clients. However, after a while the system utilization grew to a point where reads from the database were timing out.
Our database administrators pored over logs to try and figure out what was going wrong. In the end, they discovered that because there were no rate limits set for anything on the system, processes were stomping all over each other. Disk contention would spike to 100% and remain there as different processes attempted to read data from different parts of the disk. This in turn led to a kind of sadistic round-robin timeout-retry loop. Jobs would never complete.
A system was devised to place limits on the number of connections possible on the database, and rate limits were placed on bits per second a connection could read, and the problems went away. Clients had to wait longer for their jobs to complete, but they completed, and we were able to perform proper capacity planning to expand the capacity of the system in a structured way.
Rate limits allow you to reason about the performance and stability of your system by preventing it from falling outside the boundaries you’ve already investigated. If you need to expand those boundaries, you can do so in a controlled manner after lots of testing and coffee.
In scenarios where you’re charging for access to your system, rate limits can maintain a healthy relationship with your clients. You can allow them to try the system out under heavily constrained rate limits. Google does this with its cloud offerings to great success.
After they’ve become paying customers, rate limits can even protect your users. Because most of the time access to the system is programmatic, it’s very easy to introduce a bug that accesses your paid system in a runaway manner. This can be a very costly mistake and leaves both parties in the awkward situation of deciding what to do: does the service owner eat the cost and forgive the unintended access, or is the user forced to pay the bill, which might sour the relationship permanently?
Rate limits are often thought of from the perspective of people who build the resources being limited, but rate limiting can also be utilized by users. If I’m only just understanding how to utilize a service’s API, it would be very comforting to be able to scale the rate limits way down so I know I won’t shoot myself in the foot.
Hopefully I’ve given enough justification to convince you that rate limits are good even if you set limits that you think will never be reached. They’re pretty simple to create, and they solve so many problems that it’s hard to rationalize not using them.
So how do we go about implementing rate limits in Go?
Most rate limiting is done by utilizing an algorithm called the token bucket. It’s very easy to understand, and relatively easy to implement as well. Let’s take a look at the theory behind it.
Let’s assume that to utilize a resource, you have to have an access token for the resource. Without the token, your request is denied. Now imagine these tokens are stored in a bucket waiting to be retrieved for usage. The bucket has a depth of d, which indicates it can hold d access tokens at a time. For example, if the bucket has a depth of five, it can hold five tokens.
Now, every time you need to access a resource, you reach into the bucket and remove a token. If your bucket contains five tokens, and you access the resource five times,
you’d be able to do so; but on the sixth try, no access token would be available. You either have to queue your request until a token becomes available, or deny the request.
Here’s a time table to help visualize the concept. time represents the time-delta in seconds, bucket represents the number of request tokens in the bucket, and a tok in the request column denotes a successful request. (In this and future time tables, we’ll assume the requests are instantaneous to simplify the visualization.)

You can see that we’re able to make all five requests before the first second, and then we are blocked as no more tokens are available for use.
So far, this is pretty straightforward. What about replenishing the tokens; do we ever get new ones? In the token bucket algorithm, we define r to be the rate at which tokens are added back to the bucket. It can be one a nanosecond, or one a minute. This becomes what we commonly think of as the rate limit: because we have to wait until new tokens become available, we limit our operations to that refresh rate.
Here’s an example of a token bucket with a depth of one, and a rate of 1 tokens/ second:
You can see that we’re immediately able to make a request, but we are then limited to one request every other second. Our rate limitation is working beautifully!
So we now have two settings we can fiddle with: how many tokens are available for immediate use—d, the depth of the bucket—and the rate at which they are replen‐ ished—r. Between these two we can control both the burstiness and overall rate limit. Burstiness simply means how many requests can be made when the bucket is full.
Here’s an example of a token bucket with a depth of five, and a rate of 0.5 tokens/ second:
